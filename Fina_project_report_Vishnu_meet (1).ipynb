{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Single Object Trackers in OpenCV: A Benchmark\n",
    "#### Vishnu vardhan reddy\n",
    "#### Meet Patel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INTRODUCTION: \n",
    "*********************************************************************************************************************\n",
    "#### AIM : \n",
    "Object tracking is one of the fundamental tasks\n",
    "in computer vision. It is used almost everywhere: human\u0002computer interaction, video surveillance, medical treatments,\n",
    "robotics, smart cars, etc. Many object tracking methods have\n",
    "been published in recent scientific publications. However, many\n",
    "questions still remain unanswered, such as, which object tracking\n",
    "method to choose for a particular application considering some\n",
    "specific characteristics of video content or which method will\n",
    "perform the best (quality-wise) and which one will have the\n",
    "best performance? In this paper, we provide some insights into\n",
    "how to choose an object tracking method from the widespread\n",
    "OpenCV library. We provide benchmarking results on the OTB\u0002100 dataset by evaluating the eight trackers from the OpenCV\n",
    "library. We use two evaluation methods to evaluate the robustness\n",
    "of each algorithm: OPE and SPE combined with Precision and\n",
    "Success Plot\n",
    "*********************************************************************************************************************\n",
    "#### Github Repo: \n",
    "https://github.com/adnanb97/OpenCV-Research-Benchmarking/blob/master/Single%20Object%20Trackers%20in%20OpenCV%20A%20Benchmark.pdf\n",
    "*********************************************************************************************************************\n",
    "#### DESCRIPTION OF PAPER:\n",
    "The goal of object tracking is to estimate the state of the\n",
    "selected object in the subsequent frames. The object being\n",
    "tracked is usually marked using a rectangle to indicate its\n",
    "location in the starting1\n",
    "frame. When there are no changes\n",
    "in the environment, object tracking is not overly complex,\n",
    "but this is rarely the case. Various disturbances are a normal\n",
    "occurrence in the real world. These disturbances might include\n",
    "occlusion, variations in illumination, change of viewpoint,\n",
    "rotation, blurring due to motion, etc. The task of designing a\n",
    "robust and efficient tracker is known to be a very challenging\n",
    "task.\n",
    "*********************************************************************************************************************\n",
    "#### PROBLEM STATEMENT :\n",
    "To track the obejcts in the images and pdf\n",
    "*********************************************************************************************************************\n",
    "#### CONTEXT OF THE PROBLEM:\n",
    ".\n",
    "*********************************************************************************************************************\n",
    "#### SOLUTION:\n",
    "Data Collection and Preprocessing:\n",
    "Gather a diverse and comprehensive dataset containing examples of hate speech, offensive language, and neutral content. Preprocess the data by cleaning and tokenizing the text to prepare it for analysis.\n",
    "\n",
    "Text Classification Models: \n",
    "Develop state-of-the-art text classification models using deep learning architectures like Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs), or Transformer-based models (such as BERT or GPT). These models can be trained to differentiate between hate speech and non-hateful content.\n",
    "\n",
    "Multi-Modal Approaches:\n",
    "Combine textual analysis with other modalities like images and videos to detect hate speech that may not be solely reliant on text.\n",
    "\n",
    "Semi-Supervised Learning: \n",
    "Incorporate semi-supervised learning techniques to make the most of limited labeled data, leveraging unlabeled data to improve model performance.\n",
    "\n",
    "Fine-Tuning: \n",
    "Fine-tune pre-trained language models on hate speech detection tasks, enabling them to capture intricate linguistic patterns specific to hate speech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement paper code :\n",
    "*********************************************************************************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "# Directory containing the OTB-100 dataset videos\n",
    "videos_directory = \"/home/vishnu1601/Documents/Finalproject/videos\"  # Replace with the actual directory\n",
    "\n",
    "# Path to Haar Cascade classifier XML file\n",
    "face_cascade = cv2.CascadeClassifier('C:\\\\opencv\\\\build\\\\etc\\\\haarcascades\\\\haarcascade_frontalface_default.xml')\n",
    "# cascade_path = \"/home/vishnu1601/Documents/Finalproject/cascade.xml\"  # Replace with the actual path\n",
    "\n",
    "# List all video files in the directory\n",
    "video_files = [f for f in os.listdir(videos_directory) if f.endswith('.avi') or f.endswith('.mp4')]\n",
    "\n",
    "# Loop through each video file\n",
    "for video_file in video_files:\n",
    "    video_path = os.path.join(videos_directory, video_file)\n",
    "\n",
    "    # Read the video file\n",
    "    video_capture = cv2.VideoCapture(video_path)\n",
    "\n",
    "    # Loop through the frames and perform object tracking\n",
    "    while True:\n",
    "        ret, frame = video_capture.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Convert frame to grayscale for Haar Cascade detection\n",
    "        gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        # Detect objects using the Haar Cascade\n",
    "        objects = face_cascade.detectMultiScale(gray_frame, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "        \n",
    "        for (x, y, w, h) in objects:\n",
    "            cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "        \n",
    "        # Display the frame with detected objects\n",
    "        cv2.imshow(\"Object Detection\", frame)\n",
    "        \n",
    "        # Press 'q' to quit the video\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # Release the video capture and close windows\n",
    "    video_capture.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Analysis and Exploration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Dataset link:http://cvlab.hanyang.ac.kr/tracker_benchmark/datasets.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in /Users/dhavalpatel/anaconda3/lib/python3.10/site-packages (4.8.0.76)\n",
      "Requirement already satisfied: numpy>=1.21.4 in /Users/dhavalpatel/anaconda3/lib/python3.10/site-packages (from opencv-python) (1.23.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: textblob in /Users/dhavalpatel/anaconda3/lib/python3.10/site-packages (0.17.1)\n",
      "Requirement already satisfied: nltk>=3.1 in /Users/dhavalpatel/anaconda3/lib/python3.10/site-packages (from textblob) (3.7)\n",
      "Requirement already satisfied: joblib in /Users/dhavalpatel/anaconda3/lib/python3.10/site-packages (from nltk>=3.1->textblob) (1.1.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/dhavalpatel/anaconda3/lib/python3.10/site-packages (from nltk>=3.1->textblob) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in /Users/dhavalpatel/anaconda3/lib/python3.10/site-packages (from nltk>=3.1->textblob) (4.64.1)\n",
      "Requirement already satisfied: click in /Users/dhavalpatel/anaconda3/lib/python3.10/site-packages (from nltk>=3.1->textblob) (8.0.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from textblob import TextBlob\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*********************************************************************************************************************\n",
    "### Contribution  Code :\n",
    "* This code showcases a contribution to the field of data analysis and clustering by demonstrating how to perform multi-modal clustering using text and temporal features. By combining different types of information, the code aims to identify patterns and group similar data points together. The code is designed as a starting point for researchers or practitioners interested in exploring data-driven insights without requiring explicit labels. It illustrates the potential of multi-modal approaches to uncover hidden relationships within the data and provides a foundation for further exploration and refinement based on specific objectives and datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results :\n",
    "*******************************************************************************************************************************\n",
    "Each cluster seems to represent a distinct theme or research area, with the clustering algorithm grouping papers based on similarity in their features. The content and trends within each cluster can provide insights into the variety of topics that emerge from the dataset and offer a glimpse into the underlying patterns and directions of research in the given field.\n",
    "\n",
    "#### Observations :\n",
    "*******************************************************************************************************************************\n",
    "We carefully analyzed the dataset, adeptly visualized clusters, and thoughtfully introduced meaningful labels. This process enhanced our understanding, revealing clear research themes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion and Future Direction :\n",
    "*******************************************************************************************************************************\n",
    "#### Learnings : \n",
    "\n",
    "\n",
    "In conclusion, this code introduces a valuable contribution to data analysis and clustering by exemplifying the application of multi-modal clustering, amalgamating text and temporal features. Through the synergy of diverse data facets, it facilitates pattern recognition and cohesive grouping of akin data points. Offering a springboard for label-free data exploration, the code underscores multi-modal strategies' potency in revealing concealed data relationships. Its potential to unveil latent insights and provide a preliminary framework for tailored refinement cements its significance for researchers and practitioners alike.\n",
    "\n",
    "*******************************************************************************************************************************\n",
    "#### Results Discussion :\n",
    "\n",
    "The code produces notable outcomes, merging text and temporal features for successful pattern identification and data point grouping. Its label-free adaptability underscores practicality, emphasizing multi-modal strategies for uncovering intricate data relationships and encouraging future exploration.\n",
    "*******************************************************************************************************************************\n",
    "#### Limitations :\n",
    "\n",
    "However, there are some limitations to consider. The code's effectiveness heavily relies on the quality and relevance of the text and temporal features, which might vary in different datasets. Additionally, the performance of the clustering results could be influenced by the choice of similarity measures and clustering algorithms used in the code. Finally, the code's scalability to large datasets might need further optimization to maintain efficiency and accuracy.\n",
    "\n",
    "*******************************************************************************************************************************\n",
    "#### Future Extension :\n",
    "Furthermore, integrating advanced machine learning techniques, like deep learning or semi-supervised learning, could enhance clustering accuracy and versatility. Exploring online, incremental clustering approaches could also enable real-time analysis of streaming data, broadening the code's applicability to dynamic datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References:\n",
    "\n",
    "[1]:  Jahan, M.S., & Oussalah, M. (2021). Title of the Paper. Journal of Information Management, Volume(Issue), Page numbers. DOI: https://www.sciencedirect.com/science/article/pii/S0925231223003557?via%3Dihub\n",
    "\n",
    "[2]: Google. (2019). Hate speech policy. YouTube Help. URL: https://support.google.com/youtube/answer/2801939?hl=en\n",
    "\n",
    "[3]: Jahan, M.S. (2021). Google_sholars_ACM_digital_library_crawler. GitHub. URL: https://github.com/saroarjahan/Google_sholars_ACM_digital_library_crawler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
